<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
	<title>UCB/LBL Applied Math Seminar &#8211; Rachel Ward &#8211; Stochastic Gradient Descent: Strong convergence guarantees &ndash; without parameter tuning</title>
	<base href="http://math.lbl.gov/ams/" />
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<link rel="stylesheet" type="text/css" media="screen, projection" href="default.css" />
	<meta name="description" content="A joint seminar series covering a wide variety of topics in
	applied mathematics, PDEs, and scientific computation." />
	<meta name="keywords" content="Berkeley, applied math, applied, mathematics, LBL, LBNL, Lawrence, National, Lab, Laboratory, seminar, talks, PDE, numerics, numerical, mathematical, methods, computation, fluid, mechanics, dynamics, simulation, scientific" />
	<meta name="author" content="Chris Rycroft" />
	<meta name="robots" content="all" />
	<link href="index.html" rel="start" title="Seminar Index" />
</head>
<body>
	<div id="container">
		<div id="head">
			<div id="maintitle">
				<h1><a href="index.html">Applied Mathematics Seminar</a></h1>
			<h2>UC Berkeley / Lawrence Berkeley Laboratory</h2>
			</div>
<!--
			<h3>Organized by <a href="https://math.berkeley.edu/~linlin/">Lin Lin</a>,<br/><a href="http://math.lbl.gov/~mjzahr/">Matthew J. Zahr</a>,<br/> and <a href="http://persson.berkeley.edu/">Per-Olof Persson</a><br/> </h3>
-->
			<h3>Organized by <a href="https://math.berkeley.edu/people/faculty/sun-ica-ani-0">Suncica Canic</a>,<br/><a href="https://math.berkeley.edu/~linlin/">Lin Lin</a>,<br/> and <a href="http://persson.berkeley.edu/">Per-Olof Persson</a> <br/>  </h3>
		</div>
<h4 class="top">Stochastic Gradient Descent: Strong convergence guarantees &ndash; without parameter tuning</h4>
<h5><b>Rachel Ward, University of Texas, Austin</b></h5>
<h5>August 30th, 4PM-5PM, Evans 60, 2018 at 11:00AM&ndash;12:00PM in Evans 732</h5>
<p>Stochastic Gradient Descent is the basic optimization algorithm behind powerful deep learning architectures which are becoming increasingly omnipresent in society.  However, existing theoretical guarantees of convergence rely on knowing certain properties of the optimization problem such as maximal curvature and noise level which are not known a priori in practice.  Thus, in practice, hyper parameters of the algorithm such as the stepsize are tuned by hand before training, taking days or weeks.  In this talk, we discuss a modification of Stochastic Gradient Descent with an adaptive "on the fly" step size update known as AdaGrad which is used in practice but until now did not come with any theoretical guarantees.  We provide the first such guarantees, showing that Stochastic Gradient Descent with AdaGrad converges to a near-stationary point of a smooth loss function, at a rate which nearly matches the "oracle" rate as if the curvature of the loss function and noise level on the stochastic gradients were known in advance.  We also  demonstrate its favorable empirical performance on deep learning problems compared to pre-tuned state-of-the-art algorithms.
</p>

<br />
	<div id="foot">
		<div class="links">
			<a href="http://math.berkeley.edu/">UCB Math</a> |
			<a href="http://math.lbl.gov/">LBL Math</a>
		</div>
	</div>
</div>
</body>
</html>
