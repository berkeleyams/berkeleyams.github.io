Semester: Fall 2022
Usual location: 939 Evans (for in-person talks) and <a href="https://berkeley.zoom.us/j/186935273">https://berkeley.zoom.us/j/186935273</a>
Usual time: 4:10PM-5PM
Usual day: Wednesday





Date: 9/7
Host: Lin/Di
Speaker: Mo Zhou
Note: Virtual
Affiliation: Duke University
Title: Neural network approaches for high dimensional problems
<abstract>
Neural networks are effective tools for solving high dimensional problems. In this talk, I will summarize the popular methods to solve high dimensional problems with neural networks. Then I will briefly introduce two of my works based on the DeepBSDE method. In the first work, we solve the eigenvalue problem by transforming it into a fixed-point formulation, which is a diffusion Monte Carlo like approach. In the second work, we leverage the actor-critic framework from reinforcement learning to solve the static Hamilton—Jacobi—Bellman equations. We propose a variance reduced temporal difference method for the critic and apply an adaptive step size algorithm for the actor to improve accuracy.
</abstract>



Date: 9/14
Host: Michael
Speaker: Krutika Tawri
Note: in-person
Affiliation: University of California, Berkeley
Title: On stochastic partial differential equations with a Ladyzenskaya-Smagorinsky type nonlinearity
<abstract>
The theory of monotone operators plays a central role in many areas of nonlinear analysis. Monotone operators often appear in fluid dynamics, for example the p-Laplacian appears in a non-Newtonian variant of the Navier-Stokes equations modeled by Ladyzenskaya or in the Smagorinsky model of turbulence. In this talk, we will discuss global existence results of both martingale and pathwise solutions of stochastic equations with a monotone operator, of the Ladyzenskaya-Smagorinsky type, driven by a general Levy noise. The classical approach based on using directly the Galerkin approximation is not valid. In this talk we will discuss how one can approximate a monotone operator by a family of monotone operators acting in a Hilbert space, so as to recover certain useful properties of the orthogonal projectors and overcome the challenges faced while applying the Galerkin scheme.
</abstract>



Date: 9/21
Host: Di	
Note: Virtual
Speaker: Li Wang 
Affiliation: University of Minnesota
Title: Variational methods for gradient flow
<abstract>
In this talk, I will introduce a general variational framework for nonlinear evolution equations with a gradient flow structure, which arise in material science, animal swarms, chemotaxis, and deep learning, among many others. Building upon this framework, we develop numerical methods that have built-in properties such as positivity preserving and entropy decreasing, and resolve stability issues due to the strong nonlinearity. Two specific applications will be discussed. One is the Wasserstein gradient flow, where the major challenge is to compute the Wasserstein distance and resulting optimization problem. I will show techniques to overcome these difficulties. The other is to simulate crystal surface evolution, which suffers from significant stiffness and therefore  prevents simulation with traditional methods on fine spatial grids. On the contrary, our method resolves this issue and is proved to converge at a rate independent of the grid size. 
</abstract>



Date: 9/28
Host: Di
Note: Virtual
Speaker: Felix Leditzky
Affiliation: UIUC
Title: The platypus of the quantum channel zoo
<abstract>
Understanding quantum channels and the strange behavior of their capacities is a key driver of quantum information theory. Despite having rigorous coding theorems, quantum capacities are poorly understood due to super-additivity effects. We will talk about a remarkably simple, low-dimensional, single-parameter family of quantum channels with exotic quantum information-theoretic features. As the simplest example from this family, we focus on a qutrit-to-qutrit channel that is intuitively obtained by hybridizing together a simple degradable channel and a completely useless qubit channel. Such hybridizing makes this channel's capacities behave in a variety of interesting ways. For instance, the private and classical capacity of this channel coincide and can be explicitly calculated, even though the channel does not belong to any class for which the underlying information quantities are known to be additive. Moreover, the quantum capacity of the channel can be computed explicitly, given a clear and compelling conjecture is true. This "spin alignment conjecture," which may be of independent interest, is proved in certain special cases and additional numerical evidence for its validity is provided. We further show that this qutrit channel demonstrates superadditivity when transmitting quantum information jointly with a variety of assisting channels, in a manner unknown before. A higher-dimensional variant of this qutrit channel displays super-additivity of quantum capacity together with an erasure channel. Subject to the spin-alignment conjecture, our results on super-additivity of quantum capacity extend to lower-dimensional channels as well as larger parameter ranges. In particular, super-additivity occurs between two weakly additive channels each with large capacity on their own, in stark contrast to previous results. Remarkably, a single, novel transmission strategy achieves super-additivity in all examples. Our results show that super-additivity is much more prevalent than previously thought. It can occur across a wide variety of channels, even when both participating channels have large quantum capacity.

This is joint work with Debbie Leung, Vikesh Siddhu, Graeme Smith, and John Smolin, and based on the papers https://arxiv.org/abs/2202.08380 and https://arxiv.org/abs/2202.08377.

</abstract>



Date: 10/5
Host: Lin
Speaker: Chao Ma
Affiliation: Stanford University
Title: Implicit bias of optimization algorithms for neural networks and their effects on generalization
<abstract>
Modern neural networks are usually over-parameterized—the number of parameters exceeds the number of training data. In this case the loss functions tend to have many (or even infinite) global minima, which imposes an additional challenge of minima selection on optimization algorithms besides the convergence. Specifically, when training a neural network, the algorithm not only has to find a global minimum, but also needs to select minima with good generalization among many other bad ones. In this talk, I will share a series of works studying the mechanisms that facilitate global minima selection of optimization algorithms. First, with a linear stability theory, we show that stochastic gradient descent (SGD) favors flat and uniform global minima. Then, we build a theoretical connection of flatness and generalization performance based on a special structure of neural networks. Next, we study the global minima selection dynamics—the process that an optimizer leaves bad minima for good ones—in two settings. For a manifold of minima around which the loss function grows quadratically, we derive effective exploration dynamics on the manifold for SGD and Adam, using a quasistatic approach. For a manifold of minima around which the loss function grows subquadratically, we study the behavior and effective dynamics for GD, which also explains the edge of stability phenomenon. 
</abstract>


Date: 10/12
Location: 939 Evans
Note: in-person
Host: Michael
Speaker: Mark Fornace
Affiliation: Caltech
Title: 
<abstract>
</abstract>


Date: 10/19
Note: in-person
Location: 939 Evans
Host: Di
Speaker: Li Gao
Affiliation: University of Houston
Title: 
<abstract>
</abstract>


Date: 10/24 
Note: 4:10PM-5PM in-person. Note the special date!
Location: 939 Evans
Host: Lin
Speaker: Matthew Colbrook
Affiliation: Cambridge
Title: 
<abstract>
</abstract>


Date: 11/2
Host: Di
Speaker: Yonah Borns-Weil 
Note: in-person
Affiliation: University of California, Berkeley
Title: 
<abstract>
</abstract>



Date: 11/9
Host: Franziska
Note: in-person
Location: 939 Evans
Speaker: Samuel Lanthaler
Affiliation: Caltech
Title: 
<abstract>
</abstract>


Date: 11/16
Host: Franziska
Note: in-person
Speaker: Lucas Bouck
Affiliation: Univ of Maryland
Title: 
<abstract>
</abstract>


Date: 11/23
Host: 
Speaker: No seminar. Happy Thanksgiving!
Affiliation: 
Title: 
<abstract>
</abstract>



Date: 11/30
Host: Di	
Note: in-person
Speaker: Sui Tang
Affiliation: UCSB
Title: 
<abstract>
</abstract>



