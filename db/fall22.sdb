Semester: Fall 2022
Usual location: 939 Evans (for in-person talks) and <a href="https://berkeley.zoom.us/j/186935273">https://berkeley.zoom.us/j/186935273</a>
Usual time: 4:10PM-5PM
Usual day: Wednesday





Date: 9/7
Host: Lin/Di
Speaker: Mo Zhou
Note: Virtual
Affiliation: Duke University
Title: Neural network approaches for high dimensional problems
<abstract>
Neural networks are effective tools for solving high dimensional problems. In this talk, I will summarize the popular methods to solve high dimensional problems with neural networks. Then I will briefly introduce two of my works based on the DeepBSDE method. In the first work, we solve the eigenvalue problem by transforming it into a fixed-point formulation, which is a diffusion Monte Carlo like approach. In the second work, we leverage the actor-critic framework from reinforcement learning to solve the static Hamilton—Jacobi—Bellman equations. We propose a variance reduced temporal difference method for the critic and apply an adaptive step size algorithm for the actor to improve accuracy.
</abstract>



Date: 9/14
Host: Michael
Speaker: Krutika Tawri
Note: in-person
Affiliation: University of California, Berkeley
Title: On stochastic partial differential equations with a Ladyzenskaya-Smagorinsky type nonlinearity
<abstract>
The theory of monotone operators plays a central role in many areas of nonlinear analysis. Monotone operators often appear in fluid dynamics, for example the p-Laplacian appears in a non-Newtonian variant of the Navier-Stokes equations modeled by Ladyzenskaya or in the Smagorinsky model of turbulence. In this talk, we will discuss global existence results of both martingale and pathwise solutions of stochastic equations with a monotone operator, of the Ladyzenskaya-Smagorinsky type, driven by a general Levy noise. The classical approach based on using directly the Galerkin approximation is not valid. In this talk we will discuss how one can approximate a monotone operator by a family of monotone operators acting in a Hilbert space, so as to recover certain useful properties of the orthogonal projectors and overcome the challenges faced while applying the Galerkin scheme.
</abstract>



Date: 9/21
Host: Di	
Note: Virtual
Speaker: Li Wang 
Affiliation: University of Minnesota
Title: Variational methods for gradient flow
<abstract>
In this talk, I will introduce a general variational framework for nonlinear evolution equations with a gradient flow structure, which arise in material science, animal swarms, chemotaxis, and deep learning, among many others. Building upon this framework, we develop numerical methods that have built-in properties such as positivity preserving and entropy decreasing, and resolve stability issues due to the strong nonlinearity. Two specific applications will be discussed. One is the Wasserstein gradient flow, where the major challenge is to compute the Wasserstein distance and resulting optimization problem. I will show techniques to overcome these difficulties. The other is to simulate crystal surface evolution, which suffers from significant stiffness and therefore  prevents simulation with traditional methods on fine spatial grids. On the contrary, our method resolves this issue and is proved to converge at a rate independent of the grid size. 
</abstract>



Date: 9/28
Host: Di
Note: Virtual
Speaker: Felix Leditzky
Affiliation: UIUC
Title: The platypus of the quantum channel zoo
<abstract>
Understanding quantum channels and the strange behavior of their capacities is a key driver of quantum information theory. Despite having rigorous coding theorems, quantum capacities are poorly understood due to super-additivity effects. We will talk about a remarkably simple, low-dimensional, single-parameter family of quantum channels with exotic quantum information-theoretic features. As the simplest example from this family, we focus on a qutrit-to-qutrit channel that is intuitively obtained by hybridizing together a simple degradable channel and a completely useless qubit channel. Such hybridizing makes this channel's capacities behave in a variety of interesting ways. For instance, the private and classical capacity of this channel coincide and can be explicitly calculated, even though the channel does not belong to any class for which the underlying information quantities are known to be additive. Moreover, the quantum capacity of the channel can be computed explicitly, given a clear and compelling conjecture is true. This "spin alignment conjecture," which may be of independent interest, is proved in certain special cases and additional numerical evidence for its validity is provided. We further show that this qutrit channel demonstrates superadditivity when transmitting quantum information jointly with a variety of assisting channels, in a manner unknown before. A higher-dimensional variant of this qutrit channel displays super-additivity of quantum capacity together with an erasure channel. Subject to the spin-alignment conjecture, our results on super-additivity of quantum capacity extend to lower-dimensional channels as well as larger parameter ranges. In particular, super-additivity occurs between two weakly additive channels each with large capacity on their own, in stark contrast to previous results. Remarkably, a single, novel transmission strategy achieves super-additivity in all examples. Our results show that super-additivity is much more prevalent than previously thought. It can occur across a wide variety of channels, even when both participating channels have large quantum capacity.

This is joint work with Debbie Leung, Vikesh Siddhu, Graeme Smith, and John Smolin, and based on the papers https://arxiv.org/abs/2202.08380 and https://arxiv.org/abs/2202.08377.

</abstract>



Date: 10/5
Host: Lin
Speaker: Chao Ma
Note: in-person
Affiliation: Stanford University
Title: Implicit bias of optimization algorithms for neural networks and their effects on generalization
<abstract>
Modern neural networks are usually over-parameterized—the number of parameters exceeds the number of training data. In this case the loss functions tend to have many (or even infinite) global minima, which imposes an additional challenge of minima selection on optimization algorithms besides the convergence. Specifically, when training a neural network, the algorithm not only has to find a global minimum, but also needs to select minima with good generalization among many other bad ones. In this talk, I will share a series of works studying the mechanisms that facilitate global minima selection of optimization algorithms. First, with a linear stability theory, we show that stochastic gradient descent (SGD) favors flat and uniform global minima. Then, we build a theoretical connection of flatness and generalization performance based on a special structure of neural networks. Next, we study the global minima selection dynamics—the process that an optimizer leaves bad minima for good ones—in two settings. For a manifold of minima around which the loss function grows quadratically, we derive effective exploration dynamics on the manifold for SGD and Adam, using a quasistatic approach. For a manifold of minima around which the loss function grows subquadratically, we study the behavior and effective dynamics for GD, which also explains the edge of stability phenomenon. 
</abstract>


Date: 10/12
Location: 939 Evans
Note: in-person
Host: Michael
Speaker: Mark Fornace
Affiliation: Caltech
Title: Theoretical methods for nucleic acid secondary structure thermodynamics and kinetics
<abstract>
Nucleic acid secondary structure models offer a simplified but powerful lens through which to view, analyze, and design nucleic acid chemistry. Computational approaches based on such models are central to current research directions across molecular programming and the life sciences more broadly. Considering only structures involving noncrossing partitions of nucleotides, dynamic programming algorithms can exactly compute equilibrium quantities (with respect to an approximate free energy model) in cubic complexity. I first show how such algorithms may be improved in speed, augmented in accuracy, and unified across a variety of physical quantities.

While analysis and design paradigms for nucleic acid thermodynamics are long-established in essence, nucleic acid kinetics have proved vexing for accurate and principled estimation algorithms. Past approaches have thus generally relied on stochastic simulation of the respective continuous time Markov chains (an asymptotically correct but computationally costly approach). In contrast, I show how a principled Galerkin-type approach to the kinetics proves remarkably amenable to deterministic estimation by dynamic programming algorithms. While inexact, the approach proves empirically accurate and is theoretically extensible to treatments of mass-action kinetics, macrostate models, and sequence design.
</abstract>


Date: 10/19
Note: in-person
Location: 939 Evans
Host: Di
Speaker: Li Gao
Affiliation: University of Houston
Title: Logarithmic Sobolev inequalities for matrices and matrix-valued functions
<abstract>
Logarithmic Sobolev inequalities, first introduced by Gross in 70s, have rich connections to probability, geometry, as well as information theory. In recent years, logarithmic Sobolev inequalities for quantum Markov semigroups attracted a lot of attentions for its applications in quantum information theory and quantum many-body systems. In this talk, I'll present a simple, information-theoretic approach to modified logarithmic Sobolev inequalities for both quantum Markov semigroup on matrices, and classical Markov semigroup on matrix-valued functions. In the classical setting, our results implies every sub-Laplacian of a Hörmander system admits a uniform  modified logarithmic Sobolev constant for all its matrix valued functions. For quantum Markov semigroups, we improve a previous result of Gao and Rouzé by replacing the dimension constant by its logarithm. This talk is based on a joint work with Marius Junge, Nicholas, LaRacunte, and Haojian Li.
</abstract>


Date: 10/24 
Note: 4:10PM-5PM in-person. Note the special date!
Location: 939 Evans
Host: Lin
Speaker: Matthew Colbrook
Affiliation: Cambridge
Title: Residual Dynamic Mode Decomposition: Rigorous Data-Driven Computation of Spectral Properties of Koopman Operators for Dynamical Systems
<abstract>
Koopman operators are infinite-dimensional operators that globally linearize
nonlinear dynamical systems, making their spectral information valuable for
understanding dynamics. However, Koopman operators can have continuous
spectra, can lack finite-dimensional invariant subspaces, and approximations can
suffer from spectral pollution (spurious modes). These issues make computing
the spectral properties of Koopman operators a considerable challenge. This two-
part talk will detail the first scheme (ResDMD) with convergence guarantees for
computing the spectra and pseudospectra of general Koopman operators from
snapshot data. Furthermore, we use the resolvent operator and ResDMD to
compute smoothed approximations of spectral measures (including continuous
spectra), with explicit high-order convergence. ResDMD is similar to extended
DMD, except it rigorously concurrently computes a residual from the same
snapshot data, allowing practitioners to gain confidence in the computed results.
Kernelized variants of our algorithms allow for dynamical systems with a high-
dimensional state-space, and the error control provided by ResDMD allows a
posteriori verification of learnt dictionaries. We apply ResDMD to compute the
spectral measure associated with the dynamics of a protein molecule (20,046-dimensional state-space) and demonstrate several problems in fluid dynamics
(with state-space dimensions > 100,000). For example, we compare ResDMD
and DMD for particle image velocimetry data from turbulent wall-jet flow, the
acoustic signature of laser-induced plasma, and turbulent flow past a cascade of
aerofoils.
</abstract>


Date: 11/2
Host: Di
Speaker: Yonah Borns-Weil 
Note: in-person
Affiliation: University of California, Berkeley
Title: 
<abstract>
</abstract>



Date: 11/9
Host: Franziska
Note: in-person
Location: 939 Evans
Speaker: Samuel Lanthaler
Affiliation: Caltech
Title: 
<abstract>
</abstract>


Date: 11/16
Host: Franziska
Note: in-person
Speaker: Lucas Bouck
Affiliation: Univ of Maryland
Title: 
<abstract>
</abstract>


Date: 11/23
Host: 
Speaker: No seminar. Happy Thanksgiving!
Affiliation: 
Title: 
<abstract>
</abstract>



Date: 11/30
Host: Di	
Note: in-person
Speaker: Sui Tang
Affiliation: UCSB
Title: 
<abstract>
</abstract>



