Semester: Spring 2014
Usual location: 939 Evans Hall
Usual time: 3:30PM--4:30PM
Usual day: Wednesday

Date: January 22nd
Speaker: Lin Lin	
Affiliation: Lawrence Berkeley National Laboratory
Title: Fast algorithms for electronic structure analysis
<abstract>
Kohn-Sham density functional theory (KSDFT) is the most widely used electronic structure theory for molecules and condensed matter systems. For a system with N electrons, the standard method for solving KSDFT requires solving N eigenvectors for an O(N) * O(N) Kohn-Sham  Hamiltonian matrix.  The computational cost for such procedure is expensive and scales as O(N^3).  We have developed pole expansion plus selected inversion (PEXSI) method, in which KSDFT is solved by evaluating the selected elements of the inverse of a series of sparse symmetric matrices, and the overall algorithm scales at most O(N^2) for all materials including insulators, semiconductors and metals.  The PEXSI method can be used with orthogonal or nonorthogonal basis set, and the physical quantities including electron density, energy, atomic force, density of states, and local density of states are calculated accurately without using the eigenvalues and eigenvectors.  The recently developed massively parallel PEXSI method has been implemented in SIESTA, one of the most popular electronic structure software packages using atomic orbital basis sets.  The resulting method can allow accurate treatment of electronic structure in a unprecedented scale.   We demonstrate the application of the method for solving graphene-like structures with more than 30,000 atoms, and the method can be efficiently parallelized 10,000 - 100,000 processors on Department of Energy (DOE) high performance machines.
</abstract>


Date: January 29th
Speaker: TBA 

Date: February 5th
Speaker: Mickael Chekroun
Affiliation: University of California, Los Angeles 
Title: Non-Markovian Reduced Equations for Stochastic PDEs 
<abstract>
In this talk, a novel approach to deal with the parameterization problem of the ``small" scales by the ``large" ones for stochastic partial differential equations will be discussed. This approach relies on stochastic parameterizing manifolds (PMs) which are random manifolds aiming to provide --- in a mean square sense ---  approximate parameterizations of the small scales by the large ones.  Backward-forward systems will be introduced to give access to such PMs as pullback limits depending --- through the nonlinear terms ---  on the time-history of the dynamics of the low modes. It will be shown that the corresponding pullback limits can be efficiently determined in practice, leading in turn to an operational procedure for the  derivation of non-Markovian reduced equations able to achieve good modeling performances.  A stochastic Burgers-type equation will serve to illustrate how the memory effects conveyed by such reduced systems play a key role to reach such performances, in particular concerning the ability of these reduced systems to capture noise-induced transitions or  large excursions caused by the noise.  This talk is based on a joint work with Shouhong Wang (IU) and Honghu Liu (UCLA). 
</abstract>


Date: February 12th
Speaker: TBA

Date: February 19th
Speaker: Aleksandar Donev
Affiliation: Courant Institute of Mathematical Sciences
Title: The Truth about diffusion (in liquids)
<abstract>
We demonstrate that hydrodynamics and fluctuations affect diffusion in liquids in crucial ways, for both molecular diffusion (fluid mixtures), and colloidal suspensions.  We study diffusive mixing in the presence of thermal fluctuations when the Schmidt number is large. We obtain a closed equation for the concentration which is amenable to efficient numerical solution. This equation captures both Fick's law for the ensemble-averaged mean and also the long-range correlated giant fluctuations in individual realizations of the mixing process. These giant fluctuations, observed in experiments, are shown to be the result of the long-ranged hydrodynamic correlations among the diffusing particles. Through a combination of Eulerian and Lagrangian numerical experiments we demonstrate that mass transport in liquids can be modeled at all scales, from the microscopic to the macroscopic, not as irreversible Fickian diffusion, but rather, as reversible random advection by thermal velocity fluctuations. Our model gives effective dissipation with a diffusion coefficient that is not a material constant as its value depends on the scale of observation. Our work reveals somewhat unexpected connections between flows at small scales, dominated by thermal fluctuations, and flows at large scales, dominated by turbulent fluctuations. This is joint work with Thomas Fai and Eric Vanden-Eijnden.
</abstract>


Date: February 26th
Speaker: TBA

Date: March 5th
Speaker: Nancy Nichols
Affiliation: University of Reading 
Title: A moving-mesh technique together with data assimilation for simulating glaciers and ice sheets
<abstract>
Although the mathematics of ice sheet dynamics is well-established, the prediction of profiles and grounding movement are analytically infeasible and numerically difficult to achieve. Here we present a new ‘moving-mesh’ approach to simulating ice sheets and glaciers computationally that is driven by ice diffusion movement and successfully reproduces the features of ice-flow, including advance and retreat. The method is applied to a shallow ice model of a glacier using data from one of the EISMINT test cases and the results are compared.
We then show how assimilation of measured data can be used within the moving framework to improve the prediction of ice sheet movement. We develop a procedure for treating the mesh point positions, together with the ice thickness, as unknown state variables within the assimilation system. The correlation between the unknown mesh positions and the ice thickness is approximated by a simple correlation function that provides flow dependent co-variances. We demonstrate the success of the technique for noisy, very infrequent, partial measurements of ice thickness, both with and without noisy measurements of the position of the flow boundary.
</abstract>

Date: March 12th
Speaker: Peter Deuflhard
Affiliation: Freie Universitat Berlin 
Title: The Smile of the Mathematicians. Mathematical Modelling and Simulation in Facial Surgery.
<abstract>
Some mathematicians have a problem with a smile -- a mathematical one.

The talk will present results of a joint project with surgeons in the field
of cranio-maxillo-facial surgery. In this kind of surgery, faces are operatively changed to get rid of 
unpleasant face distortions. Bone from the upper or lower jaws is removed or shifted up to cm's. 
The questions treated by mathematicians are:

(1) operation planning within teleconferences of ZIB and the clinic,
  based on a detailed geometric model of skull and facial tissue,

(2) prediction of the facial appearance on the basis of numerical simulation of
  the patient's soft tissue, based on fast adaptive multilevel finite element
  methods for the elastomechanic partial differential equations, either the linear
  Navier-Lamé equations or nonlinear enrichments like geometric nonlinearity or nonlinear
  material laws of Ogden type.

As is well-known, nonlinear mechanics leads to nonconvex optimization. 
Extensions of affine conjugate Newton methods for convex optimization problems 
to the nonconvex case here (more precisely: polyconvex case) are given. 
During the talk, a lot of results for real patients are inserted, 
including possible effects on the smile.
</abstract>



Date: March 19th
Speaker: Jonathan Goodman
Affiliation: Courant Institute of Mathematical Sciences, NYU
Title: Affine invariant sampling
<abstract>
MCMC (Markov Chain Monte Carlo) is the main tool in much modern computation in statistics and 
statistical physics.  The auto-correlation time of an MCMC method is, roughly speaking, the number
of MCMC "sweeps" needed to product an effectively independent sample.  Variables with large 
correlations, for example, give long auto-correlation times in MCMC methods that move variables
one at a time (single variable Metropolis), or methods that propose isotropic (i.e. uncorrelated) 
multivariate moves.  These situations can be improved by affine transformations of the sample
space, chosen to reduce correlations.  We discuss two classes of MCMC samplers that are
affine invariant, which means that they have the same with or without an affine pre-conditioning.
One (with Jonathan Weare) involves ensembles of samples being updated together.  The
samplers can be affine invariant if the moves are based on relative positions of different samples.
One of these is the basis of the Emcee Hammer software package distributed by Dan Foreman 
Mackey.  The other (with Bo Zhu) is an MCMC version of Gauss Newton Marquardt Levenberg 
nonlinear least squares, which gets its affine invariance by using derivative information.
</abstract>


Date: March 27th
Speaker: Melanie Ades
Affiliation: University of Reading
Note: Thursday, 11:00-12:00pm in 939 Evans.
Title: The equivalent-weights particle filter
<abstract>
The majority of data assimilation schemes rely on linearity assumptions. However as the resolution and complexity of both the numerical models and observations increases, these linearity assumptions become less appropriate. Particle filters are a nonlinear data assimilation method that avoid the need for such assumptions and hence can represent the full posterior pdf. Unfortunately standard particle filters suffer from filter degeneracy which makes them inapplicable in high dimensional systems. Like the Implicit particle filter, the equivalent-weights particle filter is an adaptation to the standard particle filter which aims to avoid filter degeneracy and hence theoretically gives a representation of the full posterior pdf even in high dimensional systems. The difference between the Implicit particle filter and the equivalent-weights particle filter lies in the choice of proposal density with which the standard particle filter is adapted. Here the formulation of the equivalent-weights particle filter and its relation to both the SIR filter, the optimal proposal density and the Implicit particle filter is presented. It is demonstrated that filter degeneracy does not occur with the scheme in a 65,000 variable barotropic vorticity model and so consideration is given as to how well the scheme is able to represent the true posterior pdf.
</abstract>

Date: April 2nd
Speaker: Charbel Farhat
Affiliation: Stanford
Title: DGLM: A high-order discontinuous Galerkin method with Lagrange multipliers for advection diffusion problems
<abstract>
A high-order Discontinuous Galerkin method with Lagrange Multipliers (DGLM) is presented for the solution of steady and unsteady advection-diffusion problems in the high Peclet number regime. Unlike HDG methods, it operates directly on the second-order form the advection-diffusion equation and does not require stabilization. Like the Discontinuous Enrichment Method (DEM), it chooses the basis functions among the free-space solutions of the homogeneous form of the governing differential equation, and relies on Lagrange multipliers for enforcing a weak continuity of the approximated solution across the element interface boundaries. For a homogeneous problem, the design of arbitrarily high-order elements based on the proposed method is supported by a detailed mathematical analysis. For a non-homogeneous one, the approximated solution is locally decomposed into its homogeneous and particular parts. The homogeneous part is captured by the DGLM elements designed for a homogenous problem. The particular part is obtained analytically after the source term is projected onto an appropriate polynomial space. An a posteriori error estimator for the proposed method is also derived to enable adaptive mesh refinement. All theoretical results are illustrated by high-order numerical simulations of steady and unsteady problems with steep gradients.
</abstract>

Date: April 9th
Speaker: Panayiotis Papadopoulos
Affiliation: University of California, Berkeley
Title: Multiscale modeling in continuum mechanics: A connection to the Irving-Kirkwood procedure.
<abstract>
This talk describes a process for extending the classical Irving-Kirkwood procedure used in statistical mechanics to extract
local fluxes to the problem of continuum-mechanical multiscale modeling.
An application of this extended method is explored within the context
of finite element-based homogenization of solids. Expressions for stress and heat flux derived here are contrasted to those obtained using the standard Hill-Mandel approach.
</abstract>


Date: April 16th
Speaker: Jeffrey Donatelli
Affiliation: LBL and University of California, Berkeley 
Title: An Algorithmic Framework for X-ray Nanocrystallographic Reconstruction in the Presence of the Indexing Ambiguity
<abstract>
While conventional X-ray crystallography has been extensively used to determine atomic structure, its applicability is limited to objects than can be formed into large crystal samples. An appealing alternative, made possible by recent advances in light source technology, is X-ray nanocrystallography, which is able to image structures resistant to large crystallization by substituting a large ensemble of easier to build nanocrystals, which are delivered to an X-ray beam via a liquid jet. However, nanocrystallographic diffraction experiments suffer from severe shot-to-shot variability due to varying crystal sizes, orientations, and incident photon flux densities and the diffraction images are highly corrupted with noise. Autoindexing techniques, commonly used in conventional crystallography, can determine partial orientation information using Bragg peak patterns, but only up to crystal lattice symmetry. This limitation results in an ambiguity in the orientations, known as the indexing ambiguity, when the diffraction data displays less symmetry than the lattice and leads to data that appear twinned if left unresolved. Furthermore, missing phase information must be recovered to determine the imaged object's structure. An algorithmic framework is presented that utilizes a periodic analysis of both Bragg and non-Bragg data for precise autoindexing, Fourier analysis and image segmentation to reveal crystal size, multi-modal analysis coupled with scaling to correct for varying incident photon flux densities and identify structure factors, and clique analysis on a graph theoretical model of concurrency to resolve the indexing ambiguity. Additionally, the feasibility of determining structure through iterative phasing techniques, which have less experimental requirements than traditional phasing methods, is examined. Results are presented for several sets of simulated nanocrystallographic diffraction images using typical parameters and noise levels reported in current experiments.
</abstract>



Date: April 23rd
Speaker: Sam Kanner
Affiliation: University of California, Berkeley
Title: Towards the Hybrid Simulation of Floating Vertical-Axis Wind Turbines
<abstract>
Vertical-axis wind turbines (VAWTs) are becoming popular in the offshore wind energy field due to their superiority over horizontal-axis wind turbines (HAWTs) on a floating platform. Though they are more stable and easier to service than HAWTs, a single VAWT must have a more complex mooring system. A novel platform has been designed and will be tested at a model-scale using hybrid simulation. This type of simulation combines the advantages of both the experimental and computational domains, which is necessary for such a model that is subject to varying winds and waves. In order to simulate the aerodynamics of the turbines, a high-order discontinuous Galerkin ILES method is first used to estimate the forces on a single airfoil in 2D and 3D over large angles of attack at low chord Reynolds numbers. This methodology is also applied to rotating wind turbines undergoing prescribed motion. The forces on the blades are compared to experimental data as well as analytical turbine models. Finally, some results from a moving mesh method using an element flipping technique are shown. The talk is based on joint work with Per-Olof Persson and Luming Wang (UC Berkeley).
</abstract>

Date: April 30th
Speaker: Russel Caflisch
Affiliation: University of California, Los Angeles
Title: From PDEs to Information Science and Back 
<abstract>
The arrival of massive amounts of data from imaging, sensors, computation and the internet brought with it significant challenges for information science. New methods for analysis and manipulation of big data have come from many scientific disciplines. The first focus of this presentation is the application of ideas from PDEs, such as variational principles and numerical diffusion, to image and data analysis. Examples include denoising, segmentation, inpainting and texture extraction for images. The second focus is the development of new ideas in information science, such as soft-thresholding, sparsity and compressed sensing. The subsequent application of these ideas to PDEs and numerical computation is the third focus of this talk. Examples include soft-thresholding in multiscale computation, solutions with compact support and “compressed modes” for PDEs that come from variational principles, and applications to density functional theory. 
</abstract>


Date: May 7th
Speaker: Maria Cameron
Affiliation: University of Maryland
Title: Computing the asymptotic spectrum for networks representing energy landscapes
<abstract>
The concept of metastability has caused a lot of interest in recent years. 
The spectral decomposition of the generator matrix of a stochastic network 
exposes all of the transition processes in the system while it evolves toward the equilibrium.
I discuss an efficient way to compute the asymptotics for eigenvalues and eigenvectors
starting from the low lying group for networks representing energy landscapes. 
I apply this algorithm to Wales's Lennard-Jones-38 stochastic network 
with 71887 states and 119853 edges  whose underlying potential energy landscape has a 
double-funnel structure. The result turns out to be surprising at the first glance.
The concept of metastability should be applied with care to this system.
</abstract>












