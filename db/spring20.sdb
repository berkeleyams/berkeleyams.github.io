Semester: Spring 2020
Usual location: Evans 939
Usual time: 4PM-5PM
Usual day: Wednesday


Date: 1/15
Host: Di/Alexandre 
Speaker: Amir Sagiv
Affiliation: Columbia University
Title: Prediction of random and chaotic dynamics in nonlinear optics
<abstract>
The prediction of interactions between nonlinear laser beams is a longstanding open problem. A traditional assumption is that these interactions are deterministic. We have shown, however, that in the nonlinear Schrodinger equation (NLS) model of laser propagation, beams lose their initial phase information in the presence of input noise. Thus, the interactions between beams become unpredictable as well. 

Computationally, these predictions are enabled through a novel spline-based stochastic computational method. Our algorithm efficiently estimates probability density functions (PDF) that result from differential equations with random input. This is a new and general problem in numerical uncertainty-quantification (UQ), which leads to surprising results at the intersection of probability and transport theory.
</abstract>



Date: 1/22 Evans 740
Note: Note the special time and location. This is joint with the Thematic Seminar
Host: 
Speaker: Yingzhou Li
Affiliation: Duke University
Title: Coordinate Descent Methods for Full Configuration Interactions
<abstract>
The edge eigenvalue problems arise in many applications. When the dimension of the matrix is extremely large, such as in quantum many-body problems, conventional algorithms become impractical. We reformulate the problem as a non-convex optimization problem and propose a family of coordinate descent methods to address it. Based on our convergence analysis of these proposed methods, we tailored one with a deterministic compression strategy, named CDFCI, for the ground state calculation in the configuration interaction framework. If time permits, this talk also discusses other fast algorithms, including butterfly factorization, block basis factorization, solve-training framework, etc.
</abstract>


Date: 1/29  Evans 740
Note: Note the special time and location. This is joint with the Thematic Seminar
Host: 
Speaker: David Rolnick
Affiliation: University of Pennsylvania
Title: Deep neural networks: structure and function
<abstract>
Deep neural networks have revolutionized artificial intelligence in recent years but remain poorly understood. Even as algorithms based on neural networks are used to drive cars and diagnose diseases, their design continues to rely more on trial and error than mathematics. In this talk, we provide rigorous grounding for the relationship between structure and function in neural networks. A neural network represents a complicated function as the composition of many simple parts, with the structure of the network influencing what functions can be expressed and learned from data. We show that deep networks can express many functions with exponentially fewer parameters than shallow networks. We prove there exists a massive gap between the maximum complexity of the functions that a network can express and the expected complexity of the functions that it learns in practice. Building on this work, we find that the generalized hyperplane arrangements defined by neural networks allow us to reverse-engineer a network from the function it computes.
</abstract>


Date: 2/5
Host: Lin
Speaker: Yu-Hang Tang
Affiliation: LBNL
Title: A Graph-Based Kernel Method for Scientific Machine Learning
<abstract>
Machine learning and artificial intelligence are keys to transforming scientific research at the Department of Energy. However, success stories so far have concentrated on select forms of data. In this talk, I will present our recent work on revitalizing the marginalized graph kernel to enable direct machine learning on graph datasets while bypassing any explicit feature vector representation. The marginalized graph kernel provides a generic framework that is customizable for learning on diverse types of graphs. I will then introduce GraphDot, a Python package that implements the marginalized graph kernel on general-purpose GPUs. GraphDot significantly reduces the barrier for machine learning scientists to adopt the marginalized graph kernel and interoperates seamlessly with other graph and machine learning packages in Python, such as NetworkX and scikit-learn. The package also delivers thousands of times of speedups against existing CPU-only packages thanks to a set of state-of-the-art algorithms that take advantage of the sparsity and the generalized Kronecker product structure in the linear algebra form of the graph kernel. Finally, as a demonstration, I will showcase how GraphDot can enable an active learning protocol on a molecular database to quickly train models that can accurately predict the energy of molecules within a matter of minutes.
</abstract>


Date: 2/12
Host: Lin 
Speaker: Eric Neuscamann
Affiliation: UC Berkeley
Title: Variational Excited State Optimization
<abstract>
Predicting the properties of molecules and materials that have absorbed light requires knowledge about excited state wave functions.  We will discuss variational strategies and related nonlinear optimization challenges encountered in pursuit of this information, with examples drawn from both quantum chemistry and quantum Monte Carlo.
</abstract>


Date: 2/19
Host: Lin
Speaker: Roel Van Beeumen
Affiliation: LBNL
Title: A Scalable Matrix-Free Eigensolver for Studying Many-Body Localization
<abstract>
We present a scalable and matrix-free eigensolver for studying nearest-neighbor Heisenberg spin chain plus random on-site disorder models that undergo a many-body localization (MBL) transition. This type of problem is computationally challenging because its dimension grows exponentially with the physical system size, and the solve must be iterated many times to average over different configurations of the random disorder. For each eigenvalue problem, eigenvalues from different regions of the spectrum and their corresponding eigenvectors need to be computed. Traditionally, the interior eigenstates for a single eigenvalue problem are computed via the shift-and-invert Lanczos algorithm. Due to the extremely high memory footprint of the LU factorizations, this technique is not well suited for large number of spins $L$, e.g., one needs thousands of compute nodes on modern high performance computing infrastructures to go beyond $L = 24$. We propose a new matrix-free approach that does not suffer from this memory bottleneck and even allows for simulating spin chains up to $L = 24$ spins on a single compute node. We discuss the OpenMP and hybrid MPI--OpenMP implementations of matrix-free block matrix-vector operations that are the key components of the new approach. The efficiency and effectiveness of the proposed algorithm is demonstrated by computing eigenstates in a massively parallel fashion, and analyzing their entanglement entropy to gain insight into the MBL transition.
</abstract>


Date: 2/26
Host: Lin
Speaker: Guang-Hao Low
Affiliation: Microsoft Research
Title: Probing strongly correlated systems: Towards a quantum computational advantage
<abstract>
The properties of strongly correlated systems are of great interest but have often been challenging to elucidate. Some of these difficulties may be overcome by programmable digital quantum computers, which harness the quantum-mechanical nature of reality to simulate quantum systems and promise an advantage over computers rooted in classical physics. In this talk, I review developments in quantum algorithms advancing this goal, highlight the key role of physical insight such as the interaction picture and the finite speed of light driving recent progress, and point towards further challenges in quantum computation as a tool for fundamental physics.
</abstract>


Date: 3/4
Host: Di	 
Speaker: Jacob Bedrossian
Affiliation: University of Maryland 
Title: 
<abstract>
</abstract>


Date: 3/11
Host: Lin
Speaker: Andrew Childs
Affiliation: University of Maryland
Title: 
<abstract>
</abstract>


Date: 3/18
Host: Lin
Speaker: Tamara Kolda
Affiliation: Sandia National Laboratories
Title: 
<abstract>
</abstract>


Date: 3/25
Host: 
Speaker: spring break 
Title: 
<abstract>
</abstract>


Date: 4/1
Host: 
Speaker: Lin Lin
Affiliation: UC Berkeley and LBNL
Title: 
<abstract>
</abstract>


Date: 4/8
Host: 
Speaker: 
Affiliation: 
Title: 
<abstract>
</abstract>


Date: 4/15
Host: Per
Speaker: Viral Shah
Affiliation: Julia Computing
Title: TBD
<abstract>
</abstract>


Date: 4/22
Host: Lin
Speaker: Michael Mahoney
Affiliation: UC Berkeley
Title: 
<abstract>
</abstract>


Date: 4/29
Host: 
Speaker: 
Affiliation: 
Title: 
<abstract>
</abstract>


