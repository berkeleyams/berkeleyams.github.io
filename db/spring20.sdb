Semester: Spring 2020
Usual location: Evans 939
Usual time: 4PM-5PM
Usual day: Wednesday


Date: 1/15
Host: Di/Alexandre 
Speaker: Amir Sagiv
Affiliation: Columbia University
Title: Prediction of random and chaotic dynamics in nonlinear optics
<abstract>
The prediction of interactions between nonlinear laser beams is a longstanding open problem. A traditional assumption is that these interactions are deterministic. We have shown, however, that in the nonlinear Schrodinger equation (NLS) model of laser propagation, beams lose their initial phase information in the presence of input noise. Thus, the interactions between beams become unpredictable as well. 

Computationally, these predictions are enabled through a novel spline-based stochastic computational method. Our algorithm efficiently estimates probability density functions (PDF) that result from differential equations with random input. This is a new and general problem in numerical uncertainty-quantification (UQ), which leads to surprising results at the intersection of probability and transport theory.
</abstract>



Date: 1/22 Evans 740
Note: Note the special time and location. This is joint with the Thematic Seminar
Host: 
Speaker: Yingzhou Li
Affiliation: Duke University
Title: Coordinate Descent Methods for Full Configuration Interactions
<abstract>
The edge eigenvalue problems arise in many applications. When the dimension of the matrix is extremely large, such as in quantum many-body problems, conventional algorithms become impractical. We reformulate the problem as a non-convex optimization problem and propose a family of coordinate descent methods to address it. Based on our convergence analysis of these proposed methods, we tailored one with a deterministic compression strategy, named CDFCI, for the ground state calculation in the configuration interaction framework. If time permits, this talk also discusses other fast algorithms, including butterfly factorization, block basis factorization, solve-training framework, etc.
</abstract>


Date: 1/29  Evans 740
Note: Note the special time and location. This is joint with the Thematic Seminar
Host: 
Speaker: David Rolnick
Affiliation: University of Pennsylvania
Title: Deep neural networks: structure and function
<abstract>
Deep neural networks have revolutionized artificial intelligence in recent years but remain poorly understood. Even as algorithms based on neural networks are used to drive cars and diagnose diseases, their design continues to rely more on trial and error than mathematics. In this talk, we provide rigorous grounding for the relationship between structure and function in neural networks. A neural network represents a complicated function as the composition of many simple parts, with the structure of the network influencing what functions can be expressed and learned from data. We show that deep networks can express many functions with exponentially fewer parameters than shallow networks. We prove there exists a massive gap between the maximum complexity of the functions that a network can express and the expected complexity of the functions that it learns in practice. Building on this work, we find that the generalized hyperplane arrangements defined by neural networks allow us to reverse-engineer a network from the function it computes.
</abstract>


Date: 2/5
Host: Lin
Speaker: Yu-Hang Tang
Affiliation: LBNL
Title: A Graph-Based Kernel Method for Scientific Machine Learning
<abstract>
Machine learning and artificial intelligence are keys to transforming scientific research at the Department of Energy. However, success stories so far have concentrated on select forms of data. In this talk, I will present our recent work on revitalizing the marginalized graph kernel to enable direct machine learning on graph datasets while bypassing any explicit feature vector representation. The marginalized graph kernel provides a generic framework that is customizable for learning on diverse types of graphs. I will then introduce GraphDot, a Python package that implements the marginalized graph kernel on general-purpose GPUs. GraphDot significantly reduces the barrier for machine learning scientists to adopt the marginalized graph kernel and interoperates seamlessly with other graph and machine learning packages in Python, such as NetworkX and scikit-learn. The package also delivers thousands of times of speedups against existing CPU-only packages thanks to a set of state-of-the-art algorithms that take advantage of the sparsity and the generalized Kronecker product structure in the linear algebra form of the graph kernel. Finally, as a demonstration, I will showcase how GraphDot can enable an active learning protocol on a molecular database to quickly train models that can accurately predict the energy of molecules within a matter of minutes.
</abstract>


Date: 2/12
Host: Lin 
Speaker: Eric Neuscamann
Affiliation: UC Berkeley
Title: Variational Excited State Optimization
<abstract>
Predicting the properties of molecules and materials that have absorbed light requires knowledge about excited state wave functions.  We will discuss variational strategies and related nonlinear optimization challenges encountered in pursuit of this information, with examples drawn from both quantum chemistry and quantum Monte Carlo.
</abstract>


Date: 2/19
Host: 
Speaker: 
Affiliation: 
Title: 
<abstract>
</abstract>


Date: 2/26
Host: 
Speaker: 
Affiliation: 
Title: 
<abstract>
</abstract>


Date: 3/4
Host: Di	 
Speaker: Jacob Bedrossian
Affiliation: University of Maryland 
Title: 
<abstract>
</abstract>


Date: 3/11
Host: Lin
Speaker: Andrew Childs
Affiliation: University of Maryland
Title: 
<abstract>
</abstract>


Date: 3/18
Host: Lin
Speaker: Tamara Kolda
Affiliation: Sandia National Laboratories
Title: 
<abstract>
</abstract>


Date: 3/25
Host: 
Speaker: spring break 
Title: 
<abstract>
</abstract>


Date: 4/1
Host: 
Speaker: 
Affiliation: 
Title: 
<abstract>
</abstract>


Date: 4/8
Host: Lin
Speaker: Michael Mahoney
Affiliation: UC Berkeley
Title: 
<abstract>
</abstract>


Date: 4/15
Host: Per
Speaker: Viral Shah
Affiliation: Julia Computing
Title: TBD
<abstract>
</abstract>


Date: 4/22
Host: 
Speaker: 
Affiliation: 
Title: 
<abstract>
</abstract>


Date: 4/29
Host: 
Speaker: 
Affiliation: 
Title: 
<abstract>
</abstract>


