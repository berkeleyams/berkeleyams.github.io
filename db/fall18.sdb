Semester: Fall 2018
Usual location: Evans 732
Usual time: 11:00AM--12:00PM
Usual day: Thursday

Date: August 30th, 4PM-5PM, Evans 60
Note: Note the special time and location. This is joint with the Math Colloquium and Stat Seminar
Speaker: Rachel Ward
Affiliation: University of Texas, Austin
Title: Stochastic Gradient Descent: Strong convergence guarantees -- without parameter tuning 
<abstract>
Stochastic Gradient Descent is the basic optimization algorithm behind powerful deep learning architectures which are becoming increasingly omnipresent in society.  However, existing theoretical guarantees of convergence rely on knowing certain properties of the optimization problem such as maximal curvature and noise level which are not known a priori in practice.  Thus, in practice, hyper parameters of the algorithm such as the stepsize are tuned by hand before training, taking days or weeks.  In this talk, we discuss a modification of Stochastic Gradient Descent with an adaptive "on the fly" step size update known as AdaGrad which is used in practice but until now did not come with any theoretical guarantees.  We provide the first such guarantees, showing that Stochastic Gradient Descent with AdaGrad converges to a near-stationary point of a smooth loss function, at a rate which nearly matches the "oracle" rate as if the curvature of the loss function and noise level on the stochastic gradients were known in advance.  We also  demonstrate its favorable empirical performance on deep learning problems compared to pre-tuned state-of-the-art algorithms.
</abstract>

Date: September 6th
Speaker: Matthew J. Zahr
Affiliation: Lawrence Berkeley National Laboratory
Title: Integrated computational physics and numerical optimization
<abstract>
Optimization problems governed by partial differential equations are
ubiquitous in modern science, engineering, and mathematics. They
play a central role in optimal design and control of multiphysics systems,
data assimilation, and inverse problems. However, as the complexity of
the underlying PDE increases, efficient and robust methods to accurately
compute the objective function and its gradient become paramount. To
this end, I will present a globally high-order discretization of PDEs and
their quantities of interest and the corresponding fully discrete adjoint method
for use in a gradient-based PDE-constrained optimization setting. The framework
is applied to solve a slew of optimization problems including the design of
energetically optimal flapping motions, the design of energy harvesting
mechanisms, and data assimilation to dramatically enhance the resolution of
magnetic resonance images. In addition, I will demonstrate that the role of
optimization in computational physics extends well beyond these traditional
design and control problems. I will introduce a new method for the discovery
and high-order accurate resolution of shock waves in compressible flows using
PDE-constrained optimization techniques. The key feature of this method is an
optimization formulation that aims to align discontinuous features of the
solution basis with the discontinuities in the solution. The method is
demonstrated on a number of one- and two-dimensional transonic and supersonic
flow problems. In all cases, the framework tracks the discontinuity closely
with curved mesh elements and provides accurate solutions on extremely coarse
meshes.
</abstract>

Date: September 13th
Speaker: Youngsoo Choi
Affiliation: Lawrence Livermore National Laboratory
Title: ST-GNAT and SNS: Model order reduction techniques for time-dependent nonlinear system of equations
<abstract>
Several reduced order models have been successfully developed for nonlinear dynamical systems. To achieve a considerable speedup, a hyper-reduction step is needed to reduce the computational complexity due to nonlinear terms.  A new space–time reduced order model, the ST-GNAT method, for nonlinear dynamical systems will be introduced as well as the traditional methods, such as the DEIM and GNAT methods. The ST-GNAT method applies a space–time least-squares Petrov–Galerkin projection and space–time gappy POD approach to reduce both the dimensionality and complexity of the system. An attractive error bound associated with the ST-GNAT method and several compelling numerical results will be shown. One drawback of the ST-GNAT method is the computationally expensive offline phase where solution and nonlinear term bases as well as corresponding sample elements are constructed. To reduce the offline cost, the SNS method is developed. In contrast to the traditional hyper-reduction techniques where collection of nonlinear term snapshots is required, the SNS method completely avoids the use of the nonlinear term snapshots. Instead, it uses the solution snapshots that are used for building a solution basis.  Furthermore, it avoids an extra data compression of nonlinear term snapshots.  As a result, the SNS method provides a more efficient offline strategy than the traditional model order reduction techniques, such as the DEIM, GNAT, and ST-GNAT methods. Numerical results support that the accuracy of the solution from the SNS method is comparable to the traditional methods.
</abstract>

Date: September 20th
Speaker: No seminar this week

Date: September 27th
Speaker: Tzanio Kolev
Affiliation: Lawrence Livermore National Laboratory
Title: Scalable High-Order Finite Elements for Compressible Hydrodynamics
<abstract>
The discretization of the Euler equations of gas dynamics ("compressible hydrodynamics") in a moving material frame is at the heart of many multi-physics simulation algorithms. The Arbitrary Lagrangian-Eulerian (ALE) framework is frequently applied in these settings in the form of a Lagrange phase, where the hydrodynamics equations are solved on a moving mesh, followed by a three-part "advection phase" involving mesh optimization, field remap and multi-material zone treatment.

This talk presents a general Lagrangian framework [1] for discretization of compressible shock hydrodynamics using high-order finite elements. The novelty of our approach is in the use of high-order polynomial spaces to define both the mapping and the reference basis functions. This leads to improved robustness and symmetry preservation properties, better representation of the mesh curvature that naturally develops with the material motion, significant reduction in mesh imprinting, and high-order convergence for smooth problems. We also discuss ongoing work on the application of the curvilinear technology to the “advection phase” of ALE, including a DG-advection approach for conservative and monotonic high-order finite element interpolation (remap), high-order extensions of classical mesh optimization algorithms, such as harmonic and equipotential smoothing and the use of high-order material indicator function for handling mixed elements in multi-material ALE problems.

In addition to their mathematical benefits, high-order finite element discretizations are a natural fit for future HPC hardware, because their order can be used to tune the performance, by increasing the FLOPs/bytes ratio, or to adjust the algorithm for different hardware. In this direction, we present some of our work on scalable high-order finite element software that combines the modular finite element library MFEM [2], the hypre library of linear solvers [3], and the high-order shock hydrodynamics code BLAST [4]. We explain how the MPI-based version of MFEM uses data structures and kernels from the hypre library to enable scalable finite element assembly in parallel and describe the efficient implementation of high-order force matrices in the MFEM-based BLAST application, where we will also demonstrate the benefits of our approach with respect to strong scaling and GPU acceleration. We also consider general non-conforming high-order adaptive refinement in MFEM with applications to compressible hydrodynamics in BLAST and computational electromagnetic problems.

Finally, we give a brief overview of the related efforts in the co-design Center for Efficient Exascale Discretizations (CEED) in the Exascale Computing Project (ECP) of the DOE [5].

[1] "High-Order Curvilinear Finite Element Methods for Lagrangian Hydrodynamics", V. Dobrev and Tz. Kolev and R. Rieben, SIAM Journal on Scientific Computing, (34) 2012, pp.B606-B641.
[2] MFEM: Modular finite element library, http://mfem.org.
[3] hypre: Scalable linear solvers library, http://llnl.gov/casc/hypre.
[4] BLAST: High-order shock hydrodynamics, http://llnl.gov/casc/blast.
[5] Center for Efficient Exascale Discretizations, http://ceed.exascaleproject.org.
</abstract>

Date: October 4th
Speaker: Weizhu Bao
Affiliation: National University of Singapore
Title: Computational methods for the dynamics of the nonlinear Schroedinger/Gross-Pitaevskii equations 
<abstract>
In this talk, I begin wtih the nonlinear Schroedinger/Gross-Pitaevskii equations (NLSE/GPE) for modeling Bose-Einstein condensation (BEC), nonlinear optics, quantum physics and chemistry, etc., and review some dynamical properties of NLSE/GPE including conserved quantities, dispersion relation, center-of-mass dynamics, soliton solutions and semiclassical limits. Different numerical methods will be presented including finite different time domain (FDTD) methods and time-splitting spectral method, and their error estimates and comparison will be discussed. Extensions to NLSE/GPE with an angular momentum rotation term and/or non-local dipole-dipole interaction as well as multi-component will be presented. Finally, applications to soliton interactions, collapse and explosion of BEC, quantum transport and quantized vortex interaction will be investigated.
</abstract>


Date: October 11th
Speaker: Karthik Duraisamy
Affiliation: University of Michigan
Title: Reduced order modeling of multiscale problems using the Mori-Zwanzig formalism
<abstract>
This talk will address the issue of closure in reduced order models (ROMs) and large eddy simulations (LES), leveraging ideas from non-equilibrium statistical mechanics. The approach is based on the Variational Multi-Scale method (VMS) and the Mori-Zwanzig (M-Z) formalism, which provides a framework to perform formal scale separation and re-cast a high-dimensional dynamical system into an equivalent, lower-dimensional system. In this reduced system, which is in the form of a generalized Langevin equation (GLE), the effect of the unresolved modes on the resolved modes appears as a convolution integral (which is sometimes referred to as memory). The M-Z formalism alone does not lead to a reduction in computational complexity as it requires the solution of the orthogonal dynamics PDE. A model for the memory is constructed by assuming that memory effects have a finite temporal support and by exploiting scale similarity. We discover that unresolved scales lead to memory effects that are driven by an orthogonal projection of the coarse-scale residual and inter-element jumps (in the case of discontinuous finite elements). It is further shown that an MZ-based finite memory model is a variant of the well-known adjoint-stabilization method. For hyperbolic equations, this stabilization is shown to have the form of an artificial viscosity term. We further establish connections between the memory kernel and approximate Riemann solvers. In the context of ROMs, this model is shown to yield a Petrov-Galerkin projection. Several applications in ROMs and LES ranging from simple scalar PDEs to Magneto-hydro-dynamic turbulence will be presented.

Bio: Karthik Duraisamy is an Associate Professor of Aerospace Engineering at the University of Michigan, Ann Arbor. He obtained a doctorate in aerospace engineering and masters in applied mathematics from the University of Maryland, College Park.  He is the director of the Center for Data-driven Computational Physics and the associate director of the Michigan Institute of Computational Discovery and Engineering (MICDE) at the Univ of Michigan. His research interests are in data-driven and reduced order modeling, turbulence modeling and simulations, and numerical methods for PDEs.
</abstract>

Date: October 18th
Speaker: Jianxin Zhu
Affiliation: Los Alamos National Laboratory
Title: Dynamical mean-field theory to strongly correlated electronic systems
<abstract>
Electronic correlation effects play an import role in emergent phenomena such as Mott-insulator-metal transition and unconventional superconductivity. Understanding these effects present a theoretical challenge. In this talk, we will give an overview of dynamical mean-field theory (DMFT) and its combination with the local density approximation in density functional theory. Representative quantum impurity solvers including continuous-time quantum Monte Carlo method will also be discussed, together with a few measurable quantities.  Finally, I will present applications of the theoretical approach to strongly correlated f-electron systems.
</abstract>

Date: October 25th
Speaker: Samuel Rudy
Affiliation: University of Washington
Title: Data-driven methods for model discovery and approximation
<abstract>
A critical challenge in many modern scientific disciplines is deriving governing equations and forecasting models from data where derivation from first principals is intractable.  The problem of learning dynamics from data is complicated when data is corrupted by noise, when only partial or indirect knowledge of the state is available, when dynamics exhibit parametric dependencies, or when only small volumes of data are available.  In this talk I will discuss several methods for constructing models of dynamical systems from data including sparse identification for ordinary differential equations, sparse identification for partial differential equations with or without parametric dependencies, and approximation of dynamical systems governing equations using neural networks.  Limitations of each approach and future research directions will be discussed.
</abstract>

Date: November 1st
Speaker: Yuan Yao
Affiliation: Hong Kong University of Science and Technology
Title: Rethinking Generalization and Robustness in Neural Networks: Breiman’s Dilemma and Huber’s Model
<abstract>
We approach the following two fundamental problems in deep learning: (a) how can over-parameterized models generalize well in neural networks?
(b) how does deep learning achieve the robustness against adversarial samples?

For problem (a), Max-Margin has been an important strategy since perceptrons in machine learning for the purpose of boosting the robustness of 
classifiers toward a good generalization ability, which experienced a renaissance lately to explain the success in deep learning. However, Leo Breiman 
pointed out a dilemma in 1999 that margin increase over training data results in a decrease in generalization performance, that will be shown ubiquitous in 
neural networks as well. In particular, we propose a new method to explain the mechanism of Breiman’s Dilemma, using phase transitions of normalized 
margin dynamics.

For problem (b), we revisit Huber’s contamination model in robust statistics, from a perspective of generative adversarial networks (GAN). When the outlier 
examples are fully agnostic in distributions, GANs are shown in both theory and experiment to achieve robust estimates at information-theoretically optimal rates, 
equivalent in statistical precision to the Tukey median estimate that is NP-hard to compute though. GANs may have wider adaptation than other polynomial 
algorithms proposed lately based on moment methods. Hence, by playing some zero-sum differential games, GANs provides us provable guarantees on 
robustness under Huber’s model. 
</abstract>

Date: November 8th
Speaker: Yuehaw Khoo
Affiliation: Stanford University
Title: Convex optimization for multimarginal optimal transport problem with Coulomb cost
<abstract>
 We introduce methods from convex optimization to solve the multimarginal transport problem arise in the context of strictly correlated electron density functional theory. Convex relaxations are used to provide outer approximation to the set of $N$-representable 2-marginals and 3-marginals, which in turn provide lower bounds to the energy. We further propose rounding schemes based on tensor decomposition to obtain upper bounds to the energy. Numerical experiments demonstrate a gap of order $10^{-3}$ to $10^{-2}$ between the upper and lower bounds. 
</abstract>


Date: November 15th
Speaker: Tengyu Ma
Affiliation: Stanford University
Title: Recent Progress in the Theory of Deep Learning
<abstract>
A deeper understanding of the principles of deep learning can consolidate and boost its already-spectacular empirical success. I will introduce some of the recent progress in the theory of deep learning, including some of my own work.  We will discuss the core ML issues, such as optimization, generalization, and expressivity, and their rich interactions, in the contexts of supervised learning with (deep) non-linear models. 
</abstract>

Date: November 22th
Speaker: Thanksgiving break

Date: November 29th
Speaker: Alexandre Chorin
Affiliation: LBNL, UC Berkeley
Title: Renormalization and large eddy simulation for a driven Burgers equation in a hydrodynamic regime
<abstract>
A real-space renormalization group (RNG) is constructed for a randomly-driven Burgers equation, with irrelevant degrees of freedom eliminated sequentially by stochastic parametrization followed by scaling. The connection with more standard implementations of an RNG is spelled out. The parameters in the equation and in the forcing, as well as the construction of the RNG, are chosen so that the resulting random process resembles the one in hydrodynamic turbulence, where the forcing acts on the largest scales and ``universality" appears in the intermediate (``inertial") scales. The output of the construction is a discrete model that describes the motion at the coarsest scales in terms of these scales alone, as in large eddy simulation.  An example is presented, which exhibits a RNG parameter flow with an inertial range. The broader significance of the results is discusssed. (joint work with Fei Lu)
</abstract>

Date: December 6th
Speaker: Xiuyuan Cheng
Affiliation: Duke University
Title: Convolutional Neural Network with Decomposed Filters 
<abstract>
Filters in a Convolutional Neural Network (CNN) contain model parameters learned from data. The properties of convolutional filters in a trained deep network directly affect the quality of the feature representation being learned. In this talk, we introduce a framework for decomposing convolutional filters over a truncated expansion under pre-fixed bases, where the expansion coefficients are adaptive. Such a structure not only reduces the number of trainable parameters and computational load but also imposes filter regularity by bases truncation. Apart from maintaining prediction accuracy across image classification datasets, the decomposed-filter CNN also produces a stable representation with respect to input variations proved under generic assumptions. The framework extends to group-equivariant CNNs where it significantly reduces the model complexity and demonstrates improved stability of the trained network. Joint work with Qiang Qiu, Robert Calderbank, and Guillermo Sapiro. 
</abstract>

