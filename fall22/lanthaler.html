<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
	<title>UCB/LBL Applied Math Seminar &#8211; Samuel Lanthaler &#8211; Supervised learning in function space</title>
	<base href="https://berkeleyams.lbl.gov/" />
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<link rel="stylesheet" type="text/css" media="screen, projection" href="default.css" />
	<meta name="description" content="A joint seminar series covering a wide variety of topics in
	applied mathematics, PDEs, and scientific computation." />
	<meta name="keywords" content="Berkeley, applied math, applied, mathematics, LBL, LBNL, Lawrence, National, Lab, Laboratory, seminar, talks, PDE, numerics, numerical, mathematical, methods, computation, fluid, mechanics, dynamics, simulation, scientific" />
	<meta name="author" content="Chris Rycroft" />
	<meta name="robots" content="all" />
	<link href="index.html" rel="start" title="Seminar Index" />
</head>
<body>
	<div id="container">
		<div id="head">
			<div id="maintitle">
				<h1><a href="index.html">Applied Mathematics Seminar</a></h1>
			<h2>UC Berkeley / Lawrence Berkeley Laboratory</h2>
			</div>

<!--			<h3>Organized by <a href="https://math.berkeley.edu/people/faculty/sun-ica-ani-0">Suncica Canic</a>,<br/><a href="https://math.berkeley.edu/~difang/">Di Fang</a>,<br/><a href="https://math.berkeley.edu/~linlin/">Lin Lin</a>,<br/> and <a href="http://persson.berkeley.edu/">Per-Olof Persson</a> <br/>  </h3>
-->
			<h3>Organized by <a href="https://math.berkeley.edu/~difang/">Di Fang</a>,<br/><a href="https://quantumtative.github.io/">Michael Lindsey</a>,<br/> and <a href="https://sites.google.com/andrew.cmu.edu/franzisw/home">Franziska Weber</a> <br/>  </h3>
		</div>
<h4 class="top">Supervised learning in function space</h4>
<h5><b>Samuel Lanthaler, Caltech</b></h5>
<h5>11/9, 2022 at 4:10PM-5PM in 939 Evans</h5>
<p>Neural networks have proven to be effective approximators of high dimensional functions in a wide variety of applications. In scientific applications the goal is often to approximate an underlying operator, which defines a mapping between infinite-dimensional spaces of input and output functions. Extensions of neural networks to this infinite-dimensional setting have been proposed in recent years, giving rise to the rapidly emerging field of operator learning. Despite their practical success, our theoretical understanding of these approaches is still in its infancy. In this talk, I will review some of the proposed operator learning architectures (deep operator networks/neural operators), and present recent results on their approximation theory and sample complexity. This work identifies basic mechanisms by which neural operators can avoid a curse of dimensionality in the underlying (very high- or even infinite-dimensional) approximation task, thus providing a first rationale for their practical success for concrete operators of interest. The analysis also reveals fundamental limitations of some of these approaches.
</p>

<br />
	<div id="foot">
		<div class="links">
			<a href="http://math.berkeley.edu/">UCB Math</a> |
			<a href="http://math.lbl.gov/">LBL Math</a>
		</div>
	</div>
</div>
</body>
</html>
