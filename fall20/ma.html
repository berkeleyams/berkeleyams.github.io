<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
	<title>UCB/LBL Applied Math Seminar &#8211; Chao Ma &#8211; The Slow Deterioration of the Generalization Error of the Random Feature Model</title>
	<base href="https://berkeleyams.lbl.gov/" />
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<link rel="stylesheet" type="text/css" media="screen, projection" href="default.css" />
	<meta name="description" content="A joint seminar series covering a wide variety of topics in
	applied mathematics, PDEs, and scientific computation." />
	<meta name="keywords" content="Berkeley, applied math, applied, mathematics, LBL, LBNL, Lawrence, National, Lab, Laboratory, seminar, talks, PDE, numerics, numerical, mathematical, methods, computation, fluid, mechanics, dynamics, simulation, scientific" />
	<meta name="author" content="Chris Rycroft" />
	<meta name="robots" content="all" />
	<link href="index.html" rel="start" title="Seminar Index" />
</head>
<body>
	<div id="container">
		<div id="head">
			<div id="maintitle">
				<h1><a href="index.html">Applied Mathematics Seminar</a></h1>
			<h2>UC Berkeley / Lawrence Berkeley Laboratory</h2>
			</div>

<!--			<h3>Organized by <a href="https://math.berkeley.edu/people/faculty/sun-ica-ani-0">Suncica Canic</a>,<br/><a href="https://math.berkeley.edu/~difang/">Di Fang</a>,<br/><a href="https://math.berkeley.edu/~linlin/">Lin Lin</a>,<br/> and <a href="http://persson.berkeley.edu/">Per-Olof Persson</a> <br/>  </h3>
-->
			<h3>Organized by <a href="https://math.berkeley.edu/~difang/">Di Fang</a>,<br/><a href="https://quantumtative.github.io/">Michael Lindsey</a>,<br/> and <a href="https://sites.google.com/andrew.cmu.edu/franzisw/home">Franziska Weber</a> <br/>  </h3>
		</div>
<h4 class="top">The Slow Deterioration of the Generalization Error of the Random Feature Model</h4>
<h5><b>Chao Ma, Stanford University</b></h5>
<h5>9/2, 2020 at 4:10PM-5PM in <a href="https://berkeley.zoom.us/j/186935273">https://berkeley.zoom.us/j/186935273</a></h5>
<p>The random feature model exhibits a kind of resonance behavior when the number of parameters is close to the training sample size. This behavior is characterized by the appearance of large generalization gap, and is due to the occurrence of very small eigenvalues for the associated Gram matrix. In this paper, we examine the dynamic behavior of the gradient descent algorithm in this regime. We show, both theoretically and experimentally, that there is a dynamic self-correction mechanism at work: The larger the eventual generalization gap, the slower it develops, both because of the small eigenvalues. This gives us ample time to stop the training process and obtain solutions with good generalization property.
</p>

<br />
	<div id="foot">
		<div class="links">
			<a href="http://math.berkeley.edu/">UCB Math</a> |
			<a href="http://math.lbl.gov/">LBL Math</a>
		</div>
	</div>
</div>
</body>
</html>
